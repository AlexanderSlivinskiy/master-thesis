\chapter{Preliminaries} \label{ch:prelim}
%\begin{itemize}
%  \item \textcolor{purple}{Explain what an "incoming transition" is}
%  \item \textcolor{purple}{What does it mean if a state can be reached? <-> it has incoming transition}
%\end{itemize}

In this chapter, we introduce the underlying definitions necessary to have an understanding of the notation and foundation on which this thesis is built on. 
To get an overview of what simple stochastic games are and how they are played we provide in Section \ref{sec:intuit} an intuition. 
In Section \ref{sec:defSG} we introduce the formal definition of simple stochastic games, which is the stochastic model we consider. 
We formalize the notion of players having strategies in Section \ref{sec:defStrat} and define the semantics of simple stochastic games in Section \ref{sec:defSemantics}. 
We then consider special subparts of the state space of stochastic games called end components in Section \ref{sec:defEC}. 
Lastly, we give short descriptions of the most prominent algorithms that solve simple stochastic games and some optimizations to them in Section \ref{sec:SGAlgos}.

\section{Intuition} \label{sec:intuit}
\input{examples/exampleSG}
A \emph{simple stochastic game} is a two-player-game introduced by Shapley in \cite{shapley} played on a graph $G$ whose vertices $\states$ are partitioned into the two sets $\maxStates$ and $\minStates$. 
Each set belongs to a player. We call the players Maximizer and Minimizer. We refer to vertices also as states.

At the beginning of the game, a token is placed on the initial vertex $\initstate$. 
The Maximizer's goal is to move the token to any \emph{target} $\target \in \fstates \subseteq \states$, 
while the Minimizer's goal is that the token never reaches any target-vertex $\target$. Stochastic games with such objectives are called \emph{reachability games}.
To move the token every vertex $s \in S$ has a finite set of actions $\Av(\state)$ that can be taken if the token is on this vertex. 
Each action causes the transition of the token to another vertex with a probability according to the probability distribution $\trans: S \to [0,1] \subset \mathbb{Q}$. 
If the token is in a Maximizer-state $s \in \maxStates$ then the Maximizer decides which action to pick. 
If the token is in a Minimizer-state $\state \in \minStates$ the Minimizer decides. Figure \ref{ex:exampleSG} provides an example of a simple stochastic game.

The problem we consider in this thesis is: \emph{Given a token in the initial state $\initstate$, 
what is the probability that the token reaches any target state $\target \in \fstates$ if both players play optimally?}

\section{Stochastic games} \label{sec:defSG}%{Basic definitions}
%\vspace*{-0.2cm}
We define now simple stochastic games, also referred to as stochastic games \cite{condonComplexity}. 
To do so, we need to introduce \emph{probability distributions}. 
A probability distribution on a finite set $X$ is a mapping $\trans: X \to [0,1]$, such that $\sum_{x\in X} \trans(x) = 1$. 
The set of all probability distributions on $X$ is denoted by $\Distributions(X)$. 
\begin{definition}[\SG]
	A \emph{stochastic game ($\SG$)} is a tuple  
	$(\states,\maxStates,\minStates,\initstate,\actions,\Av,\delta)$
	where $\states$ is a finite set of \emph{states} partitioned\footnote{I.e., $\maxStates \subseteq \states$, $\minStates \subseteq \states$, $\maxStates \cup \minStates = \states$, and $\maxStates \cap \minStates = \emptyset$.}\ into the sets $\maxStates$ and $\minStates$ of states of the player \emph{Maximizer} and \emph{Minimizer} respectively, 
	$\initstate%,\target,\sink 
	\in \states$ is the \emph{initial} state, % \emph{target} state, and \emph{sink} state, respectively, 
	$\actions$ is a finite set of \emph{actions}, 
	$\Av: \states \to 2^{\actions}$ assigns to every state a set of \emph{available} actions, and 
	$\trans: \states \times \actions \to \distributions(\states)$ is a \emph{transition function} that given a state $\state$ and an action $\action\in \Av(\state)$ yields a probability distribution over \emph{successor} states.
	
	A \emph{Markov decision process (MDP)} is a special case of $\SG$ where $\minStates = \emptyset$ or $\maxStates = \emptyset$, and a Markov chain (MC) is a special case of an MDP, where for all $\state \in \states: \abs{\Av(\state)} = 1$.
\end{definition}

We use a model of stochastic games similar to \cite{paperMaxi}.

%Every vertex $s \in S$ has a finite set of actions $\Av(\state)$ that can be taken if the token is on this vertex. A \emph{probability distribution} on a finite set $X$ is a mapping $\trans: X \to [0,1]$, such that $\sum_{x\in X} \trans(x) = 1$. Each action causes the transition of the token to another vertex with a probability according to the probability distribution $\trans: S \to [0,1] \subset \mathbb{Q}$.
%The set of all probability distributions on $X$ is denoted by $\Distributions(X)$.
%Each state-action-pair has its own probability distribution.

The set of successors that can be reached from a state $\state$ taking the action $\action \in \Av(\state)$ is described as 
$\post(\state,\action) := \set{\state' \mid \trans(\state,\action,\state') > 0}$. 
We assume that every state has at least one action it can take so that for all $\state \in \states$ it holds that  $\Av(\state) \neq \emptyset$.

%If the token is in a Maximizer-state $s \in \maxStates$ then the Maximizer may decide which action to pick. If the token is in a Minimizer-state $\state \in \minStates$ the Minimizer decides.

%\textcolor{purple}{Omit it if you clearly will not need it}
%Lastly, we use $T_\Box$ and $T_{\Circ}$ for any subset of states $T \subseteq \states$ to express the states in $T$ that belong to Maximizer and Minimizer, 
%whose states are represented in the figures as $\Box$ and $\Circ$, respectively. 
%We say that a pair of a state $\state$ in $T$ and an action $\action$ \emph{leaves} $T$ if there exists a state $\state' \notin T$ with $\trans(\state,\action,\state')>0$. 
%The state $\state'$ is referred to as \emph{exit}.

\section{Strategies} \label{sec:defStrat}
As mentioned in Section \ref{sec:intuit}, the Maximizer can select the action for states $\state \in \maxStates$, 
and the Minimizer can choose the action for states $\state \in \minStates$. 
This is formalized as \emph{strategies} with $\straa: \maxStates \to \distributions(\actions)$ being a Maximizer strategy 
and $\strab : \minStates \to \distributions(\actions)$ being a Minimizer strategy, 
such that $\straa(\state) \in \distributions(\Av(\state))$ for all $\state \in \maxStates$ and $\strab (\state) \in \distributions(\Av(\state))$ for all $\state \in \minStates$. 

Since we deal with reachability games, we will consider only \emph{memoryless positional strategies}. 
This means that for each state there is exactly one fixed action that is taken: 
$\forall \state \in \maxStates : \exists \action \in \Av(\state): \straa(\state, \action) = 1$ and $\forall \state \in \minStates : \exists \action \in \Av(\state): \strab(\state, \action) = 1$. 
For reachability games, these types of strategies are optimal~\cite{condonComplexity}.

%\textcolor{purple}{May also delete}
%A pair of positional strategies $(\straa,\strab)$ induces a Markov chain $\G[\straa,\strab]$ on $\SG$ as every state has only one action, 
%and therefore it does not matter which player the state belongs to.

\section{Reachability objective} \label{sec:defSemantics}
We have introduced now what a $\SG$ is and how to fix strategies, but it is still unclear what the objectives of the players are.

As mentioned in Section \ref{sec:intuit}, at the beginning of the game a token is placed on the initial vertex $\initstate$. 
Additionally, a subset $\fstates \subseteq \states$ is provided as input. 
The Maximizer's goal is to maximize the probability of reaching any \emph{target} $\target \in \fstates$ while the Minimizer's goal is to minimize the probability that the token reaches any target $\target$. 
Vertices from which the Maximizer can never reach any target are referred to as \emph{sinks} $\zeroSink \in \zeroSinks \subseteq \states$. 
Thus, once the token reaches any sink $\zeroSink$ the Maximizer loses the game.
%\textcolor{purple}{We refer to both targets and sinks as \emph{absorbing states}.}

Since the objective of the Maximizer is to reach a target state $\target$, we call this stochastic game a \emph{reachability game}. 
In these kinds of games, we do not care what happens after reaching any target or sink state. 
Therefore, we assume for simplicity that each target $\target$ and each sink $\zeroSink$ has only one action which is a self-loop with transition probability~1. 
Figure \ref{ex:exampleSG} provides an example of an $\SG$ with one target $\target$ and one sink $\zeroSink$.

Intuitively, we can measure how good a strategy $\straa$ for $\state$ is by the probability that the token would get from $\state$ to any target $f$ if the Minimizer is using strategy $\strab$. 
This is what we call the \emph{value $\val$ of $\state \in \states$ using $(\straa,\strab)$}:
\[
	\valstra (\state) = \sum\limits_{f \in \fstates}\reachProb (f)
\]
where $\reachProb (f)$ is the probability that $\state$ reaches $f$ in $\G[\straa,\strab]$ in arbitrary many steps. 
Note that this is a high-level definition that is based on unique probability distributions over measurable sets of infinite paths that are induced by fixing strategies \cite[Ch.~10]{BaierBook}. 
However, we do not need this level of detail for this thesis and will avoid it for easier readability. 
We define $\val (\state,\action) = \sum\limits_{\state' \in \post(\state, \action)} \trans(\state, \action, \state') \val(\state')$.

As we can expect that both Maximizer and Minimizer play as good as possible and pick the optimal strategies for their goal, 
usually one is only interested in the value of states using these optimal strategies. We define this as the \emph{(optimal) value} of $\state \in \states$.
\[
	\val(\state) = \max\limits_{\straa}\min\limits_{\strab}\valstra(s) = \min\limits_{\strab}\max\limits_{\straa}\valstra(s)
\]
The second equality is due to \cite{condonComplexity}. 
Again, this is a simplified definition derived from using the supremum and infimum instead of the maximum and minimum. 
However, as we are dealing with memoryless positional strategies, a maximum and a minimum always exist~\cite[Ch.~10]{BaierBook}.

We define the \emph{value of a $\SG$} by the probability that the Maximizer wins starting in the initial state $\initstate$. 
We are mainly interested in $\val(\initstate)$ throughout this thesis. 
Condon has shown that the complexity of solving this problem is in $\nptime$ $\cap$ co-$\nptime$~\cite{condonComplexity}.

For example in Figure \ref{ex:exampleSG} the value of the initial state $s_0$ and therefore of the $\SG$ is $\trans(s_1, b, \target)$.
This is because $s_0$ has to pick action $a$ leading into $s_1$. For the Maximizer, action $b$ is better than action $a$ as long as $\trans(s_1, b, \target) > 0$,
since action $a$ in state $s_1$ leads into $s_2$, which in turn would always pick action $a$ leading always into the $\zeroSink$.

\section{End components} \label{sec:defEC}
\input{examples/exampleEC}
Some stochastic games contain subsets $T\subseteq \states$, 
for which the players can choose strategies such that the token remains forever in one of these subsets and therefore never reaches any target or sink state. 
We call such a subset an \emph{end component (EC)}. 
Figure \ref{ex:exampleEC} illustrates a simple $\SG$ with an end component. 
If both $s_0$ and $s_1$ pick action $\action$ the token would never reach neither target $\target$ nor sink $\zeroSink$.

For the definition, we need to introduce \emph{finite paths}. 
A finite path $\path$ is a finite sequence $\path = \state<0> \action<0> \state<1> \action<1> \dots \state<k> \in (\states \times \actions)^\ast \times \states$, 
such that for every $i \in [k-1]$, $\action<i>\in \Av(\state<i>)$ and $\state<i+1> \in \post(\state<i>,\action<i>)$.
\footnote{$[l] = {1, 2, \dots , l}$, where $l \in \Naturals$}
\begin{definition}[End component (EC)]\cite{paperMaxi}\label{def:EC}
A non-empty set $T\subseteq \states$ of states is an \emph{end component (EC)} if there is a non-empty set $B \subseteq \Union_{\state \in T} \Av(s)$ of actions such that 
	\begin{enumerate}
		\item for each $\state \in T, \action \in B \intersection \Av(\state)$ we do \emph{not} have $(\state,\action) \leaves$ $T$,
		\item for each $\state, \state' \in T$ there is a finite path $\fpath = \state \action<0> \dots \action<n> \state' \in (T \times B)^* \times T$, i.e. the path stays inside $T$ and only uses actions in $B$.
	\end{enumerate}
\end{definition}
 
\vspace*{-0.1cm}
An end component $T$ is a \emph{maximal end component (MEC)} if there is no other end component $T'$ such that $T \subseteq T'$.\
Note that sinks and targets are maximal end components of size 1.\
Given a $\SG$ $\G$, the set of its MECs is denoted by $\mec(\G)$ and can be computed in polynomial time~\cite{CY95}.\

Computing the value of states that are part of an End Component that is not a sink is for many algorithms a non-trivial task that requires
additional concepts.
%A \emph{stopping game} is a $\SG$ that contains only absorbing states. 
%This implies that stopping games \emph{terminate} almost surely after a finite amount of steps and either a target state or a sink state is reached. 
%As this is not the case with non-stopping games, additional concepts are necessary to handle end components correctly.

\section{Strongly connected components}
Another important structural concept we reason about is a strongly connected component (SCC)
\begin{definition}[Strongly Connected Component (SCC)]\label{def:SCC}
A set of states $V \subseteq \states$ is \emph{strongly connected} iff for every pair of states $(\state, \state') \in (V \times V)$ there is a path from $\state$ to $\state'$ and a path from $\state'$ to $\state$.
$V$ is a \emph{strongly connected component} iff $V$ is strongly connected and maximal, i.e. there is no strongly connected set $W \subseteq \states$ such that $V \subset W$.
\end{definition}

Since every state belongs to exactly one strongly connected component, the set of strongly connected components partitions the set of states $\states$.
Decomposing $\states$ into its strongly connected components can be useful because it is possible to induce subgames of any stochastic game $\SG$ based on its
SCCs and solve $\SG$ with a divide-and-conquer approach. 
Instead of computing the values of the states in the stochastic game, one can instead compute the value of the states in the subgames.
To use this approach, we need to partition $\states$ into its SCCs, and we need to compute a topological enumeration of the SCCs to know in which order to process the components.
Tarjan's Algorithm for strongly connected components \cite{TarjansAlgorithm} does both. 
In this thesis, we refer to algorithms that use SCC decomposition to solve stochastic games as topological algorithms.

\section{Algorithms for stochastic games} \label{sec:SGAlgos}
Next, we recall some of the most prominent algorithms that can \emph{solve} stochastic games i.e. compute the value of any stochastic game.

The three most common approaches to solving stochastic games are \emph{Value Iteration}, \emph{Strategy Iteration} and \emph{Quadratic Programming}.
%In this thesis, we will focus primarily on Value iteration and Strategy iteration, since even state-of-the-art Quadratic Programming Solvers are not as performant as 
%the other options at the moment \cite{gandalf}.

\subsection{Value iteration}
To compute the value function $\val$ for an SG, the following partitioning of the state space is useful: 
firstly the goal states $\targets$ that surely reach any target state $\target \in \fstates$, 
secondly the set of \emph{sink states} that do not have a path to the target $\sinks$ and finally the remaining states $\unknownStates$.
For $\targets$ and $\sinks$ (which can be easily identified by graph-search algorithms), the value is trivially 1 respectively 0. 
Thus, the computation only has to focus on $\unknownStates$.

The well-known approach of value iteration ($\VI$) leverages the fact that $\val$ is the least fixpoint of the \emph{Bellman equations}, cf.~\cite{visurvey}:
	
\begin{align}
		\val(s) = \begin{cases}
			1 & \text{if} \ s \in \targets \\
			0 & \text{if} \ s \in \sinks \\
			\max_{a \in \Av(s)} \Big(\sum_{s' \in \states} \trans(s,a,s') \cdot \val(s') \Big) & \text{if} \ s \in \maxStates^? \\
			\min_{a \in \Av(s)} \Big(\sum_{s' \in \states} \trans(s,a,s') \cdot \val(s') \Big) & \text{if} \ s \in \minStates^?
		\end{cases}   
	\end{align}
\label{eq:bellman}

Now we define\footnote{In the definition of $\Bop$, we omit the technical detail that for goal states $s \in \targets$, the value has to remain 1. Equivalently, one can assume that all goal states are absorbing, i.e.\ only have self looping actions.} the Bellman operator $\Bop: (\states \to \Rationals) \to (\states \to \Rationals)$:% based on these equations:
	\begin{align}
		\mathsf{\Bop(f)(s)} = \begin{cases}
			\max_{a \in \Av(s)} \Big(\sum_{s' \in \states} \trans(s,a,s') \cdot f(s')\Big) & \text{if} \ s \in \maxStates \\
			\min_{a \in \Av(s)} \Big(\sum_{s' \in \states} \trans(s,a,s') \cdot f(s')\Big) & \text{if} \ s \in \minStates
		\end{cases}   
	\end{align}

Value iteration starts with the under-approximation 
\[\lb_0(s) = \begin{cases} 
	1 & \text{if} \ s \in \targets\\
	0 & \text{otherwise} \end{cases}\]
and repeatedly applies the Bellman operator. Since the value is the least fixpoint of the Bellman equations and $\lb_0 \leq \val$ is lower than the value, this converges to the value in the limit~\cite{visurvey} (formally $\lim_{i \to \infty} \Bop^i(\lb_0) = \val$).

\subsubsection*{Bounded value iteration}
While this approach is often fast in practice, it has the drawback that it is not possible to know the current difference between $\Bop^i(\lb_0)$ and $\val$ for any given~$i$. 
To address this, one can employ \emph{bounded value iteration} ($\BVI$, also known as interval iteration~\cite{haddadmonmege,learningBased,paperMaxi})
It additionally starts from an over-approximation $\ub_0$, with $\ub_0(s) = 1$ for all $s \in \states$. 
However, applying the Bellman operator to this upper estimate might not converge to the value, but some greater fixpoint instead due to end components. 
See~\cite[Example 2]{gandalf} for an example.
To fix this issue, $\BVI$ computes which states outsides of the end component the Maximizer can reach from within the end component if Minimizer plays optimally. 
The upper bound for these states inside the end component is then be defined by the upper bound of the states outside the end component.
This approach is referred to as \emph{deflating} \cite{paperMaxi}.
With deflating, the upper bound decreases monotonically towards the least fixpoint. Once $\forall \state \in \states: \ub(s) - \lb(s) \leq \epsilon$,
$\BVI$ terminates and reports $\val(\initstate) = \frac{\ub_0 + \lb_0}{2}$.
In addition to $\BVI$, we will consider the widest path optimization $\WP$ introduced in \cite{widestPath} that offers an alternative to deflating for the correct handling of end components.

\subsubsection*{Optimistic value iteration}
Optimistic value iteration $\OVI$ takes a similar approach, but instead of calculating the upper bound at all times along the lower bound, 
an upper bound $B$ is guessed from the lower bound by adding a small $\epsilon$ to the lower bound. 
If this induced bound $B$ can be verified to be an upper bound, i.e. $\forall \state \in \states: \Bop(B)(\state) \leq B(\state)$, then $\OVI$ terminates.
The upper bound is only guessed if the lower bound would converge according to classic value iteration. 
If the guessed bound is not an upper bound, the precision for the lower bound gets reduced, and iterating continues.


There are more extensions to value iteration the learning-based approach introduced in \cite{learningBased}.
However, it was already benchmarked in \cite{gandalf} and its random nature is fundamentally different to the 
solution approaches we analyze in this thesis, which is why we omit it.

In practice, value iteration and its extensions are believed to be the fastest approaches to solve stochastic games.
However, as shown in \cite{haddadmonmege}, there are counterexamples where value iteration requires exponentially many steps to converge and its performance is worse than the other solution approaches like strategy iteration.

\subsection{Strategy iteration}
In Strategy iteration \cite{HoffmanKarp}\cite{condonQP}, instead of computing value-vectors we compute a sequence of strategies and terminate with an optimal strategy for both players.
Starting from an arbitrary strategy for the Maximizer, we repeatedly compute the best response of Minimizer and then greedily improve Maximizer's strategy. 
\textcolor{purple}{This is word for word copied from gandalf, so I should cite it?}
The computed sequence of Maximizer strategies is monotonically increasing regarding the value and converges to the optimal strategy \cite[Theorem3]{correctnessSI}.
For games with end components, the initial strategy may not be completely arbitrary but has to ensure that at least one target or sink is reached almost surely.
Finding such an initial strategy can be achieved with the attractor strategy \cite[Section 5.3]{correctnessSI}.

Whenever a Maximizer strategy is fixed, the resulting $\SG$ is simplified to an MDP. 
From there on, any solution method for MDPs can be applied.
We use three different variants of solving the MDPs: value iteration ($\SI$), linear programming \cite{Puterman} ($\LPSI$),
or strategy iteration ($\SISI$). When applying $\SISI$, the Minimizer picks a strategy and the MDP is reduced to a Markov chain, 
which we solve with classic exact algorithms as described in ~\cite[Chapter 11]{introProb}.

Since trying out every possible deterministic positional strategy guarantees that the optimal strategy is found,
the trivial upper bound on the number of iterations for strategy iteration is exponential in the number of actions per state.

However, so far no example can confirm that there is a stochastic game where strategy iteration truly needs exponentially many iterations.

\subsection{Quadratic programming}
In Quadratic Programming, the stochastic game is encoded into a mathematical framework called a Quadratic Program. 
The encoded problem is then handed to a solver, which uses algorithms specific to mathematical programming in general.
While solving an arbitrary quadratic problem is known to be $\NP$-complete, convex quadratic programs can be solved in polynomial time.
At the moment, it is open whether it is possible to encode every stochastic game into a convex quadratic program.
In practice, \cite{gandalf} has shown that Quadratic Programming is not competitive to Value iteration and Strategy iteration even if using 
state-of-the-art solvers. Thus, we will not include it in our benchmarks.

\subsection{Optimizations} \label{subsec:optimizations}
In addition to the algorithms and their extensions, there are various optimizations that we will consider in this thesis:

$\mathbf{G}$: The Gau{\ss}-Seidel variant of value iteration for $\BVI$ and $\OVI$. 
Bellman updates happen in-place and state by state. Thus, states updated later can use already the updated values of previous states. 

$\mathbf{D}$: The idea from~\cite{paperMaxi} to only deflate every 100 steps for $\BVI$. 
By deflating only every 100 steps, there is less time spent on searching for the end components that need to be deflated.

$\mathbf{T}$: A topological decomposition of $\SG$ into its strongly connected components for $\BVI$, $\OVI$ and $\LPSI$.
The stochastic game is decomposed into its SCCs, then the values of SCCs are computed along their topological order.
This allows the algorithms to solve smaller sub-problems before instead of solving the whole $\SG$ at once.

$\TOPAlg$: A novel variant of the topological optimization for value iteration that is unpublished at this moment \textcolor{red}{How to cite to it if unpublished?}.
To ensure that each SCC passes the correct value onto the next SCC, we use the computed value to infer strategies for the states and 
verify the correctness of the strategies by fixing them separately. This induces two MDPs. 
If passing the strategy of the opponent to policy iteration for MDPs and after one iteration the algorithm terminates for both MDPs,
we know that this must be the correct algorithm.

\subsubsection*{Algorithm Performance and Stochastic Game Structure}
As \cite{haddadmonmege} and \cite{gandalf} have shown, the performance of the algorithms is very dependent on the structural properties of the stochastic game.
Structural properties are for example the number of transitions an action has, the size of the biggest $\mec$, or the number of strongly connected components in a stochastic game.
While some algorithms may struggle with solving certain stochastic games, others may solve them without problems.
However, at the moment, there are only a few insights on which structural properties should be taken into account when deciding on which algorithm to take.
%Together with the large number of algorithms and optimizations it is a difficult task to make an informaed
%Additionally, since the introduction of stochastic games, there has been many extensions and optimizations to all three approaches, 
%making the analysis of which algorithm together with which extension to take even more difficult since there are too few criteria and comparisons
%that allow for an informed decision.

Thus, we analyze the impact of different structural properties on the performance of the algorithms that solve $\SG$.
To evaluate performance, we do not only use real case studies but also randomly generated stochastic games.