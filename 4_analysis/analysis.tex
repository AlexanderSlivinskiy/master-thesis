\chapter{Tools for algorithm analysis} \label{ch:analysis}

To facilitate the algorithm performance analysis, we track statistics about the models we use and the algorithms that solve them. 
We refer to every category of data that we track as \emph{features}.
The algorithm features we track are the time it requires to solve the problem, the iterations needed in case we use a value-iteration-based algorithm, 
and the value it has computed for correctness checks.
\textcolor{red}{Should maybe put this in a table to make it easier to reference, find and read.}
The model features we track along with their names we use in the experimental section are:

$\mathbf{State-related:}$
\begin{itemize}
\item (NumStates) Number of states
\item (NumTargets) Number of targets*
\item (NumSinks) Number of sinks*
\item (NumUnknown) Number of unknown states*
\item (NumMaxStates) Number of Maximizer states*
\item (NumMaxStates) Number of Minimizer states*
\end{itemize}

$\mathbf{Actions-related:}$
\begin{itemize}
\item (NumMaxActions) Number of maximum actions per state
\item (NumProbActions) Number of actions with probabilistic actions*
\item (AvgNumActionsPerState) Average number of actions per state
\end{itemize}

$\mathbf{Transitions-related:}$
\begin{itemize}
\item (NumMaxTransitions) Number of maximum transitions per action
\item (SmallestTransProb) Smallest occurring positive transition probability in the model
\item (AvgNumTransPerAction) Average number of transitions per action
\end{itemize} 

$\mathbf{MEC-related:}$
\begin{itemize}
\item (NumMECs) Number of MECs
\item (BiggestMEC, SmallestMEC) Size of biggest and smallest MEC*
\item (AvgMEC, MedianMEC) Average MEC-size and Median MEC-size*
\end{itemize}

$\mathbf{SCC-related:}$
\begin{itemize}
\item (NumSCCs) Number of SCCs
\item (BiggestSCC, SmallestSCC) Size of biggest and smallest SCC*
\item (AvgSCC, MedianSCC) Average SCC-size and Median SCC-size*
\item (MaxSCCDepth) Longest chain of SCCs from the initial-state SCC
\item (NumSCCNonSingleton) Number of SCCs with cardinality bigger than 1
\item (SmallestSCCNonSingleton) Smallest SCC with cardinality bigger than 1
\item (AvgSCCNonSingleton) Average SCC size with cardinality bigger than 1*
\end{itemize}

$\mathbf{Path-related}:$
\begin{itemize}
    \item (NearestTarget) Shortest Path from initial state to the nearest target
    \item (FurthestTarget) Shortest Path from initial state to the furthest target
\end{itemize}

The values of features with a star are measured absolutely, but displayed relative to a fitting measure. 
When referring to the relative value, we a \% sign instead of the "Num"-prefix (e.g. Unknown\% instead of NumUnknown).
All relative measures that are related to states like number of unknown states are displayed relative to $|\states|$ (NumStates).
We found it easier to extract knowledge from relatively displayed features due to their independence of the model size. 

\section*{Visualization}

Tracking all these features requires tools to visualize and summarize the collected data, since otherwise the raw data is overwhelming.
We have implemented a bare-bones toolset to facilitate the analysis. 

\subsubsection*{Data visualization script - data loading}
First, we load the tracked data and select the features we are interested in. We allow here to include filters to remove uninteresting data based on feature values.
We also check for errors and wrong values in the data loading phase. To assert whether a value is correct, we need a reference value whose result we believe to be true.
We usually use $\OVI$, $\TLPSI$ or $\WP$ as references since they tend to have the least timeouts and thus provide a good reference.

\subsubsection*{Data visualization script - handling missing data}
Next, we need to handle data that does not contain numerical values - for example, the algorithms that got timeouts on certain models.

One way to handle this situation is by only analyzing models where every algorithm finishes computation.
However, then we would risk losing information about models where one algorithm would perform way better than another algorithm.

Instead, we assign fixed values for missing features. For model features, we usually use 0 as a replacement value. 
If an algorithm fails to provide the correct value in time, we set its time and iterations count to a penalty value.
These penalty values should be higher than any truly occurring value to easily calculate and visualize which algorithms performed best.
For time, we usually use the time limit as penalty, and for iterations, we use 10 million iterations as penalty.

The problem of introducing these penalty values is that they influence the data distribution.
For visualization, the graphs can end up skewed because most non-penalty data points are significantly smaller than the penalty, 
and so it becomes harder to observe trends in graphs. In these cases we zoom in on the majority of the non-penalty data to be able to observe trends,
but take note of the outlying values.
Also, many visualization methods like heatmaps or statistical tests perform operations on the
data. Thus, untrue values can falsify the correlation between features. 
Thus, when working with heatmaps, we only include those models that are solved by every algorithm.