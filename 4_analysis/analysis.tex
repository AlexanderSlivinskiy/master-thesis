\chapter{Tools for algorithm analysis} \label{ch:analysis}

To facilitate the algorithm performance analysis, we track statistics about the models we use and the algorithms that solve them. 
We refer to every category of data that we track as \emph{features}.
The algorithm features we track are the time it requires to solve the problem, the iterations needed in case we use a value-iteration-based algorithm, 
and the value it has computed for correctness checks.
The model features we track along with their names we use in the experimental section are:
    \begin{longtable}{| p{0.3\linewidth} | p{0.6\linewidth} | p{0.05\linewidth} |}
        \hline
        Feature Name & Description & Rel. \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline
        \endhead
        \multicolumn{3}{|c|}{$\mathbf{State-related}$} \\\hline
        NumStates & Number of states & \\
        NumTargets & Number of targets & $\times$  \\
        NumSinks & Number of sinks & $\times$ \\
        NumUnknown & Number of unknown states & $\times$ \\
        NumMaxStates & Number of Maximizer states & $\times$ \\
        NumMinStates & Number of Minimizer states & $\times$ \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline

        \multicolumn{3}{|c|}{$\mathbf{Actions-related}$} \\\hline
        NumActions & Total number of actions & \\
        NumMaxActions & Maximal occurring number of actions per state &  \\
        NumProbActions & Number of non-deterministic actions & $\times$ \\
        AvgNumActionsPerState & Average number of actions per state & \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline

        \multicolumn{3}{|c|}{$\mathbf{Transitions-related}$} \\\hline
        NumMaxTransitions & Number of maximum transitions per action & \\
        SmallestTransProb & Smallest occurring positive transition probability in the model &  \\
        AvgNumTransPerAction & Average number of transitions per action & \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline
        \pagebreak
        \multicolumn{3}{|c|}{$\mathbf{MEC-related}$} \\\hline
        NumMECs & Number of MECs & \\
        Biggest-/ SmallestMEC & Size of biggest / smallest MEC & $\times$ \\
        AvgMEC, MedianMEC & Average MEC size and Median MEC size & $\times$ \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline

        \multicolumn{3}{|c|}{$\mathbf{SCC-related}$} \\\hline
        NumSCCs & Number of SCCs & \\
        Biggest-/ SmallestSCC & Size of biggest / smallest SCC & $\times$ \\
        AvgSCC, MedianSCC & Average SCC size and median SCC size & $\times$ \\
        Biggest-/ SmallestSCC & Size of biggest / smallest SCC & $\times$ \\
        MaxSCCDepth & Longest chain of SCCs from the SCC of the initial state & \\
        NumSCCNonSingleton & Number of SCCs with size at least 2 & \\
        SmallestSCCNonSingleton & Smallest SCC with size greater than 1 & \\
        AvgSCCNonSingleton & Average SCC size with cardinality greater than 1 & $\times$ \\\hline\multicolumn{3}{c}{}\\[-0.5em]\hline

        \multicolumn{3}{|c|}{$\mathbf{Path-related}$} \\\hline
        NearestTarget & Shortest Path from initial state to the nearest target & \\
        FurthestTarget & Shortest Path from initial state to the furthest target &  \\\hline
        \caption{A summary of all features we track in every model. If a $\times$ symbol is present in the last column of the feature,
        the feature is measured absolutely, but we display it relative to a fitting measure.}
        \label{tab:modelFeatures}
    \end{longtable}
%     \end{adjustbox}
%     \vspace{ - 05 mm}

Features with a $\times$ symbol are measured absolutely but displayed relatively. 
When referring to the relative value, we use the $\%$ sign instead of the "Num"-prefix (e.g. Unknown\% instead of NumUnknown).
Every relative feature but NumProbActions is displayed relative to $|\states|$ (NumStates). NumProbActions is displayed relative to NumActions.
We found it easier to extract knowledge from relatively displayed features due to their independence of the model size. 

\section*{Visualization}

Tracking all these features requires tools to visualize and summarize the collected data since otherwise, the raw data is overwhelming.
We have implemented a bare-bones toolset to facilitate the analysis. 

\subsubsection*{Data visualization script - data loading}
First, we load the tracked data and select the features we are interested in. We allow here to include filters to remove uninteresting data based on feature values.
We also check for errors and wrong values in the data loading phase. To assert whether a value is correct, we need a reference value whose result we believe to be true.
We usually use $\TLPSI$ or $\WP$ as references since they tend to have the least timeouts and thus provide a good reference.

\subsubsection*{Data visualization script - handling missing data}
Next, we need to handle data that does not contain numerical values - for example, the algorithms that got timeouts on certain models.

One way to handle this situation is by only analyzing models where every algorithm finishes computation.
However, then we would risk losing information about models where one algorithm would perform way better than another algorithm.

Instead, we assign fixed values for missing features. For model features, we usually use 0 as a replacement value. 
If an algorithm fails to provide the correct value in time, we set its time and iterations count to a penalty value.
These penalty values should be higher than any truly occurring value to easily calculate and visualize which algorithms performed best.
For time, we usually use the time limit as penalty, and for iterations, we use 10 million iterations as penalty.

The problem with introducing these penalty values is that they influence the data distribution.
For visualization, the graphs can end up skewed because most non-penalty data points are significantly smaller than the penalty, 
and so it becomes harder to observe trends in graphs. In these cases, we zoom in on the majority of the non-penalty data to be able to observe trends
but take note of the outlying values.
Also, many visualization methods like heatmaps or statistical tests perform operations on the
data. Thus, incorrect values can falsify the correlation between features. 
Thus, when working with heatmaps, we only include those models that are solved by every algorithm.