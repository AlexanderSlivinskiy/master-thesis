\chapter{Conclusion and future work} \label{ch:conclusion}
In this thesis, we introduced a toolset that randomly generates models to enrich the structural variety of available models.
This enabled a broader and more precise comparison of the available solution algorithms.

We have shown that our approach to generating models at random can create both data sets that share biases with the available real case studies, 
and data sets where the structural properties differ significantly from the case studies.
However, although we can generate models of arbitrary size, we are limited by the slow state exploration process of the PRISM model checker. 

%The toolset we introduced is also helpfnuul to analyze a growing set of both models and algorithms to analyze. \textcolor{red}{Sentence is wrong}
Furthermore, establishing analysis tools allowed us to prototype algorithm ideas, compare algorithm performance, 
and provide experimental evidence of a correlation between model structure and algorithm. 
Additionally, our tools for model analysis improve with increasing numbers of data points.
Although this is not the case for tables as visualization and analysis tools, 
they are nevertheless frequently the only utility used for visualization in analysis \cite{paperMaxi,widestPath,learningBased}.

Lastly, we have compared algorithms that solve $\SG$.
We found that, in general, $\WP$ is faster than every other algorithm that provides a guarantee on the precision of the value.
In our benchmarks, we could not find any class of models where $\BVI$ or $\OVI$ are consistently more performant than $\WP$.
$\WP$ is significantly better than every other algorithm at solving models with few large MECs that contain almost the whole state space.
Thus, we conclude that in case of doubt, $\WP$ is the initial algorithm we recommend.

Furthermore, we found that if a models state space is divided into a chain of SCCs, 
$\TOPAlg$ is usually the fastest algorithm to solve the problem.
We expect topological algorithms to be less performant the shorter the chains of SCCs are.

We showed that the best algorithm based on strategy iteration is $\TLPSI$.
While linear programming is inefficient for large models, 
it may be a good alternative for games that are adversarial to approaches based on value iteration.
In case $\WP$ and $\TOPAlg$ fail, we suggest using this algorithm.


\section*{Future work}
Although we have provided the first steps towards improving algorithm analysis for $\SG$s, 
there are still many fields of our work that act as a base for further improvements.

The model features we analyze at the moment helped us to draw some conclusions, but there are still many questions unanswered that would require
additional structural concepts and features that we track.
These questions are, for example, when to use Gauss-Seidel optimization or whether there is a model structure that consistently makes $\OVI$ faster than $\WP$.

In addition, there are still many data analysis techniques we did not implement yet, like stochastic tests or artificial intelligence algorithms
that can cluster features and find correlations between them. 

We believe that more research on the correlation between algorithm performance and structural properties gives way to a portfolio solver, which could first
analyze important structural features, and then pick heuristically the most adequate algorithm to solve the problem.
With enough models, the decision of which algorithm to use may be made by a neural network.
Also, combinations of algorithms like $\OVI$ and $\WP$ could be used effectively in a fitting scenario.
The user would have to pay the overhead for computing more vectors but may converge faster.

Since algorithm performance may change drastically depending on the models' size,
we need to resolve the issue that our random generation process creates large explicit files which PRISM cannot handle.
The next step would be to create an optimization system for our random generation algorithm that takes advantage of the PRISM language and holds chunks of the data in 
implicit blocks or uses modules to make the files easier parsable.

Furthermore, at the moment, our random generation algorithm enforces a tendency of states with smaller indices to have more actions than states with higher indices.
Ideally, the user should be able to remove this restriction.
This could be addressed by introducing mechanisms like, for example, that a state $\stateMac_i$ can only be connected to states $\stateMac_{j}$ with $j \in [i-50, i+50]$.
Another way to improve the random generation algorithm is to find a method of reducing the models' numbers of states that can be computed trivially.

Ultimately, improving random algorithm generation and learning more about model structure is the basis for what could become a uniform benchmarking set that could be used by everyone creating algorithms for $\SGs$ to 
test and improve their algorithms.

In conclusion, we have taken the first step towards improving algorithm analysis for SGs, 
and we have provided many promising directions for improvements of the algorithm analysis, the random model generation, and the algorithms themselves.