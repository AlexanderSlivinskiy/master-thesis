\chapter{Conclusion and future work} \label{ch:conclusion}
\textcolor{blue}{@Maxi: This Chapter is still mostly ideas.}
General outline: We have found some cool stuff, and we have guided a so far unexplored way of analyzing Data.

A huge benefit of including data analysis is that we could simply prototype an algorithm idea and assess very fast where on the spectrum of performance it lies.
Also, we can simply add more algorithms and we get an even better overview. This is not the case with the tables.
Probably we would not even have tried $\TLPSI$ if we did not have the visualization tools to simply include it.

We have shown again that structure is very important for algorithm performance.

We have created an arbitrarily big or small benchmarking suite for stochastic games that could be profitable for the whole stochastic game community.
With the random models and our analysis tool, we derived some interesting properties of algorithm X and found out Y about algorithm Z.


Most important stuff in short
\begin{itemize}
    \item Use WP. If WP is bad, try $\TLPSI$. Especially if the SCC size is fitting.
    \item We have found a nice way of creating random models - but they can only be small due to PRISM
    \item Random models we create resemble some structural properties of real models
    \item Without precomp, we can create big models with configs (hopefully)
    \item Structure is important. Scalability is too. Just because an algorithm is good on one scale, does not mean it is on the others too.
\end{itemize}

\section*{Future work}
The features we track are all very basic, but it is a good step forward. However, we still cannot explain a lot of what is going on.
For that, we may need new structural concepts and features we track.

We believe that if someone would go further down this direction, a portfolio solver is definitely possible and maybe the best solution for users.
Combinations of OVI and WP could cost a small overhead for computing both at the same time but would get the best of both.

PRISM cannot handle explicit big files.
The next step would be to create an optimization system to create random files that hold chunks of the data in implicit blocks to make the files easier parsable.

The analysis techniques could be improved by including stochastic tests and artificial intelligence algorithms to cluster features and find correlations.
This could then even allow for a portfolio solver which decides based on machine learning which algorithm to apply based on the graph structure.

At the moment, states with lower index tend do have more actions than states with higher index (33 in initial state, 4 in last states for example).
This could be addressed by introducing restrictions like for example that a state $\stateMac_i$ can only be connected to states $\stateMac_{j}$ with $j \in [i-50, i+50]$.